{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nbrg-ppcu/PhaStyle/blob/main/bin/PhaStyleExample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ProkBERT PhaStyle example\n",
        "\n",
        "The inference consists of 3 simple steps:\n",
        "\n",
        "Main steps:\n",
        " - model loading\n",
        " - prepraring the dataset (parsing fasta file and creating tokenized dataset for inference)\n",
        " - running the inference and generating the final report"
      ],
      "metadata": {
        "id": "rl2-MzEfHrF3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up the Environment\n",
        "\n",
        "While ProkBERT can operate on CPUs, leveraging GPUs significantly accelerates the process. Google Colab offers free GPU usage making it an ideal platform for trying and experimenting with ProkBERT models.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ux8JMc_dIOHJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Enabling and testing the GPU (if you are using google colab)\n",
        "\n",
        "First, you'll need to enable GPUs for the notebook:\n",
        "\n",
        "- Navigate to Edit‚ÜíNotebook Settings\n",
        "- select GPU from the Hardware Accelerator drop-down"
      ],
      "metadata": {
        "id": "5L3xq1_JIYUa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFDg_FAy1xOb"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/nbrg-ppcu/prokbert.git --quiet\n",
        "!pip install transformers datasets --quiet\n",
        "\n",
        "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "from prokbert.sequtils import *\n",
        "from prokbert.training_utils import *\n",
        "from prokbert.models import ProkBertForSequenceClassification\n",
        "from prokbert.tokenizer import LCATokenizer\n",
        "from datasets import Dataset\n",
        "\n",
        "import multiprocessing\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from os.path import join\n",
        "import os\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll confirm that we can connect to the GPU with pytorch:\n"
      ],
      "metadata": {
        "id": "nsVZeejPHjf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if CUDA (GPU support) is available\n",
        "if not torch.cuda.is_available():\n",
        "    raise SystemError('GPU device not found')\n",
        "else:\n",
        "    device_name = torch.cuda.get_device_name(0)\n",
        "    print(f'Found GPU at: {device_name}')\n",
        "num_cores = os.cpu_count()\n",
        "print(f'Number of available CPU cores: {num_cores}')"
      ],
      "metadata": {
        "id": "Q7LwqPmg2IHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing the models\n",
        "\n",
        "Downloading the model and the tokenizer\n"
      ],
      "metadata": {
        "id": "pG9pPwjgG2uV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = 'neuralbioinfo/PhaStyle-mini'\n",
        "model = ProkBertForSequenceClassification.from_pretrained(model_path, trust_remote_code=True)\n",
        "tokenizer = LCATokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "id": "81jvOY7c23Fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "%%markdown\n",
        "# Preparing the Dataset\n",
        "\n",
        "In this section, you can either **upload your own FASTA file** or use our **default EXTREMOPHILE dataset** for a quick start.\n",
        "\n",
        "1. **Upload Your FASTA**  \n",
        "   Run the next cell, then click the **Browse** button to select and upload a FASTA file from your local computer. The button will be located at the bottom of the cell.\n",
        "\n",
        "2. **Use the Default EXTREMOPHILE Dataset**  \n",
        "   If you don‚Äôt have a FASTA file handy, you can download the small **extremophiles.fasta** file, which contains the archaeal phages described in our recent paper. Simply click the link below to download and save it to your local computer (or right-click and choose ‚ÄúSave Link As‚Äù):\n",
        "\n",
        "   <a href=\"https://raw.githubusercontent.com/nbrg-ppcu/PhaStyle/main/data/EXTREMOPHILE/extremophiles.fasta\" download=\"extremophiles.fasta\">üì• Download <code>extremophiles.fasta</code></a>\n"
      ],
      "metadata": {
        "id": "R45UNfeKIoP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "if uploaded:\n",
        "    fasta_filename = list(uploaded.keys())[0]\n",
        "    print(\"Uploaded FASTA file name:\", fasta_filename)\n"
      ],
      "metadata": {
        "id": "PkVOGqJ52KNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Preprocessing\n",
        "\n",
        "Next section, we read, segment, and tokenize FASTA sequences.\n",
        "\n",
        "### Existing Steps\n",
        "1. Reading and parsing the FASTA file  \n",
        "2. Cutting long sequences into smaller segments (~512 bp)  \n",
        "3. Tokenizing and preparing for the model  "
      ],
      "metadata": {
        "id": "_mWPA3Y_Fyub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "max_length=512 # Fit to the model size\n",
        "print(f\"[prepare_dataset] Loading sequences from: {fasta_filename}\")\n",
        "\n",
        "sequences = load_contigs(\n",
        "    [fasta_filename],\n",
        "    IsAddHeader=True,\n",
        "    adding_reverse_complement=False,\n",
        "    AsDataFrame=True,\n",
        "    to_uppercase=True,\n",
        "    is_add_sequence_id=True,\n",
        ")\n",
        "print(f\"[prepare_dataset] Number of raw sequences: {len(sequences)}\")\n",
        "\n",
        "print(\"[prepare_dataset] Running segmentation\")\n",
        "segmentation_params = {\n",
        "    \"max_length\": max_length,\n",
        "    \"min_length\": int(max_length * 0.5),\n",
        "    \"type\": \"contiguous\",\n",
        "}\n",
        "raw_segment_df = segment_sequences(\n",
        "    sequences, segmentation_params, AsDataFrame=True\n",
        ")\n",
        "print(f\"[prepare_dataset] Number of segments: {len(raw_segment_df)}\")\n",
        "\n",
        "# Wrap into HF Dataset (in memory)\n",
        "hf_dataset = Dataset.from_pandas(raw_segment_df)\n",
        "\n",
        "# Tokenization function (same as before, except no labels)\n",
        "def _tokenize_fn(batch):\n",
        "    tokenized = tokenizer(\n",
        "        batch[\"segment\"],\n",
        "        padding=\"longest\",\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "    )\n",
        "    # Zero out first/last attention token\n",
        "    masks = tokenized[\"attention_mask\"]\n",
        "    for m in masks:\n",
        "        m[0] = 0\n",
        "        m[-1] = 0\n",
        "    return {\n",
        "        \"input_ids\": tokenized[\"input_ids\"],\n",
        "        \"attention_mask\": masks\n",
        "    }\n",
        "\n",
        "print(f\"[prepare_dataset] Tokenizing with {num_cores} CPU core(s)\")\n",
        "tokenized_ds = hf_dataset.map(\n",
        "    _tokenize_fn,\n",
        "    batched=True,\n",
        "    num_proc=num_cores,\n",
        "    remove_columns=hf_dataset.column_names,\n",
        "    keep_in_memory=True,\n",
        ")"
      ],
      "metadata": {
        "id": "O7G5HgQrFxab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#¬†Predicting phage lifestyle phenotype\n",
        "Now, we have the dataset which can be passed through the finetune model.\n"
      ],
      "metadata": {
        "id": "VbhE_KD9_oCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_columns = ['sequence_id', 'fasta_id', 'predicted_label', 'score_temperate', 'score_virulent']\n",
        "final_columns_rename = ['sequence_id', 'predicted_label', 'score_temperate', 'score_virulent', 'fasta_id']\n",
        "tmp_output = \"./prokbert_inference_output\"\n",
        "os.makedirs(tmp_output, exist_ok=True)\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=tmp_output,\n",
        "    do_train=False,\n",
        "    do_eval=False,\n",
        "    per_device_eval_batch_size = 32,\n",
        "    fp16=True,\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"[main] Running prediction on segments...\")\n",
        "predictions = trainer.predict(tokenized_ds)\n",
        "# Building the final table:\n",
        "\n",
        "\n",
        "final_table = inference_binary_sequence_predictions(predictions, hf_dataset)\n",
        "final_table['predicted_label'] = final_table.apply(lambda x:  'virulent' if x['predicted_label']=='class_1' else 'temperate', axis=1)\n",
        "final_table = final_table.merge(sequences[['sequence_id', 'fasta_id']], how='left',\n",
        "                                  left_on='sequence_id', right_on='sequence_id')\n",
        "#print(final_table)\n",
        "final_table.columns = final_columns_rename\n",
        "final_table = final_table[final_columns]\n",
        "\n",
        "final_table"
      ],
      "metadata": {
        "id": "PhYwzGd4GiL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enjoy! :)\n"
      ],
      "metadata": {
        "id": "E5sMOYLkBJvr"
      }
    }
  ]
}